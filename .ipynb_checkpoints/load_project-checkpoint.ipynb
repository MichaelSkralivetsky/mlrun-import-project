{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afbe7059-f788-4c7b-84fd-c2ddb20cd98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "import os\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12025c4b-9159-4614-a501-149a4d6b7d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = mlrun.load_project(name=\"load-project\", url=\"git://github.com/amit-elbaz/load_project.git\", context=\"./context\",clone=True, user_project=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b8973-4222-47da-9240-c909afc59241",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(project.to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725cd080-4012-4fe6-be68-66c8f385dea6",
   "metadata": {},
   "source": [
    "### Regular jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcec2149-aa72-4743-88f3-37587875f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    job_num = str(i)\n",
    "    \n",
    "    # Every 30 min\n",
    "    project.run_function(\"normal_job_sec\", name=f\"sched_job_sec_30m_{job_num}\", schedule=\"30 * * * *\", watch=False)\n",
    "    project.run_function(\"normal_job_min\", name=f\"sched_job_min_30m_{job_num}\", schedule=\"30 * * * *\", watch=False)\n",
    "    project.run_function(\"normal_job_hour\", name=f\"sched_job_hour_30m_{job_num}\", schedule=\"30 * * * *\", watch=False)\n",
    "    \n",
    "    # Every 1 hour\n",
    "    project.run_function(\"normal_job_sec\", name=f\"sched_job_sec_1h_{job_num}\", schedule=\"0 * * * *\", watch=False)\n",
    "    project.run_function(\"normal_job_min\", name=f\"sched_job_min_1h_{job_num}\", schedule=\"0 * * * *\", watch=False)\n",
    "    project.run_function(\"normal_job_hour\", name=f\"sched_job_hour_1h_{job_num}\", schedule=\"0 * * * *\", watch=False)\n",
    "\n",
    "    # Every 4 hours\n",
    "    project.run_function(\"normal_job_sec\", name=f\"sched_job_sec_4h_{job_num}\", schedule=\"0 */4 * * *\", watch=False)\n",
    "    project.run_function(\"normal_job_min\", name=f\"sched_job_min_4h_{job_num}\", schedule=\"0 */4 * * *\", watch=False)\n",
    "    project.run_function(\"normal_job_hour\", name=f\"sched_job_hour_4h_{job_num}\", schedule=\"0 */4 * * *\", watch=False)\n",
    "    \n",
    "    # Every 24 hours\n",
    "    project.run_function(\"normal_job_sec\", name=f\"sched_job_sec_24h_{job_num}\", schedule=\"0 0 */1 * *\", watch=False)\n",
    "    project.run_function(\"normal_job_min\", name=f\"sched_job_min_24h_{job_num}\", schedule=\"0 0 */1 * *\", watch=False)\n",
    "    project.run_function(\"normal_job_hour\", name=f\"sched_job_hour_24h_{job_num}\", schedule=\"0 0 */1 * *\", watch=False)\n",
    "\n",
    "    # Every 3 days\n",
    "    project.run_function(\"normal_job_sec\", name=f\"sched_job_sec_3d_{job_num}\", schedule=\"0 0 */3 * *\", watch=False)\n",
    "    project.run_function(\"normal_job_min\", name=f\"sched_job_min_3d_{job_num}\", schedule=\"0 0 */3 * *\", watch=False)\n",
    "    project.run_function(\"normal_job_hour\", name=f\"sched_job_hour_3d_{job_num}\", schedule=\"0 0 */3 * *\", watch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ebd676-ce13-49e3-8b86-14618e76b326",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bed013-4978-41b2-a6ae-7550eb826057",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_func = mlrun.code_to_function(name=\"spark-read\",\n",
    "                                    kind=\"spark\",\n",
    "                                    handler=\"spark_handler\",\n",
    "                                    filename=\"spark_jobs_func.py\", requirements=[\"scikit-learn\"]\n",
    "                                   ).apply(mlrun.auto_mount())\n",
    "spark_func.with_executor_requests(cpu=\"1\",mem=\"1G\")\n",
    "spark_func.with_driver_requests(cpu=\"1\",mem=\"1G\")\n",
    "spark_func.with_driver_limits(cpu=\"1\")\n",
    "spark_func.with_executor_limits(cpu=\"1\")\n",
    "spark_func.with_igz_spark()\n",
    "spark_func.spec.image_pull_policy = \"Always\"\n",
    "spark_func.spec.replicas = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e076839-0f37-4b4e-a6b5-bca5c694540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_func.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bd7fe5-4ad5-4821-84e6-f4cc42eb1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_func.run(schedule=\"0 * * * *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a87cc-fcec-4dc8-ba28-dc9c8cc693d3",
   "metadata": {},
   "source": [
    "### Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc80009-b670-435f-9fc7-8d961048834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_cluster = mlrun.new_function(\"dask-cluster\", kind='dask', image='mlrun/ml-models')\n",
    "dask_cluster.apply(mlrun.mount_v3io())        # add volume mounts\n",
    "dask_cluster.spec.service_type = \"NodePort\"   # open interface to the dask UI dashboard\n",
    "dask_cluster.spec.replicas = 1             # define one container\n",
    "dask_cluster.set_env(\"MLRUN_DBPATH\",os.environ[\"MLRUN_DBPATH\"])\n",
    "dask_cluster.set_env(\"MLRUN_DEFAULT_PROJECT\",project.name)\n",
    "uri = dask_cluster.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc0e5e-3457-4560-a2e8-db5423472acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.run_function('dask_func',hyperparams={\"i\":[1,10,20,30,40]},schedule=\"0 * * * *\", hyper_param_options={\"strategy\":\"list\",\"parallel_runs\":1,\"dask_cluster_uri\":uri,\"teardown_dask\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094d280-e246-44b8-ae20-caa50683c122",
   "metadata": {},
   "source": [
    "### Nuclio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd7497b-9840-47dd-b00c-46f7fb4542d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    job_num = str(i)\n",
    "    # Create a simple nuclio function\n",
    "    project.deploy_function(\"nuclio_func\", tag=job_num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dbc101-0565-475d-b0f8-04cfbbcf419d",
   "metadata": {},
   "source": [
    "### Serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a076b0-f053-457a-8447-448458a1c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = mlrun.get_sample_path(\"models/serving/\")\n",
    "\n",
    "suffix = (\n",
    "    mlrun.__version__.split(\"-\")[0].replace(\".\", \"_\")\n",
    "    if sys.version_info[1] > 7\n",
    "    else \"3.7\"\n",
    ")\n",
    "\n",
    "\n",
    "framework_sklearn = \"sklearn\"  # change to 'keras' to try the 2nd option\n",
    "kwargs = {}\n",
    "\n",
    "serving_class = \"mlrun.frameworks.sklearn.SklearnModelServer\"\n",
    "model_path = models_dir + f\"sklearn-{suffix}.pkl\"\n",
    "image = \"mlrun/mlrun\"\n",
    "\n",
    "model_object = project.log_model(f\"{framework_sklearn}-model\", model_file=model_path, **kwargs)\n",
    "serving_sklearn = mlrun.new_function(\"serving_sklearn\", image=image, kind=\"serving\", requirements=[\"scikit-learn\"])\n",
    "serving_sklearn.add_model(\n",
    "    framework_sklearn, model_path=model_object.uri, class_name=serving_class, to_list=True\n",
    ")\n",
    "project.deploy_function(serving_sklearn)\n",
    "\n",
    "\n",
    "framework_keras = \"keras\"\n",
    "serving_class = \"mlrun.frameworks.tf_keras.TFKerasModelServer\"\n",
    "model_path = models_dir + \"keras.h5\"\n",
    "image = \"mlrun/ml-models\"  # or mlrun/ml-models-gpu when using GPUs\n",
    "kwargs[\"labels\"] = {\"model-format\": \"h5\"}\n",
    "\n",
    "model_object = project.log_model(f\"{framework_keras}-model\", model_file=model_path, **kwargs)\n",
    "serving_tensorf = mlrun.new_function(\"serving_tensorf\", image=image, kind=\"serving\", requirements=[\"tensorflow\"])\n",
    "serving_tensorf.add_model(\n",
    "    framework_keras, model_path=model_object.uri, class_name=serving_class, to_list=True\n",
    ")\n",
    "\n",
    "project.deploy_function(serving_tensorf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac91cee-9f70-45a3-82f0-722d801b157e",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af8c87-183a-4564-963d-cdeea020bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.run(workflow_path='workflow.py',watch=True, schedule=\"0 * * * *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b8db9-585b-4149-b3ee-6343e27cca63",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4cfce6-a1aa-463a-9488-eccde4d57794",
   "metadata": {},
   "source": [
    "## Log models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f9a77b-601d-48af-b939-4db8a2086f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Function that trains a RandomForestClassifier model on the 'iris' dataset, saves it as a pickle file and returns the training set and the model path\n",
    "def create_iris_model():\n",
    "    model_path = \"iris_model.pkl\"\n",
    "    # Load dataset\n",
    "    iris = load_iris()\n",
    "    train_set = pd.DataFrame(\n",
    "        iris[\"data\"],\n",
    "        columns=[\"sepal_length_cm\", \"sepal_width_cm\", \"petal_length_cm\", \"petal_width_cm\"],\n",
    "    )\n",
    "\n",
    "    # Separate features and target variables\n",
    "    X = iris.data\n",
    "    y = iris.target\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and train the Random Forest Classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained model as pkl file\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "        \n",
    "    return train_set, model_path\n",
    "    \n",
    "\n",
    "# Function that gets the number of models to log, trains and saves a model on the 'iris' dataset and logs n models of it.\n",
    "def log_n_models(context, number_of_models):\n",
    "    train_set, model_path = create_iris_model()\n",
    "    for num in number_of_models:      \n",
    "        # Log the model\n",
    "        model_name = f\"model_num_{str(num)}\"\n",
    "        context.log_model(model_name, model_file=model_path, training_set=train_set, framework=\"sklearn\")\n",
    "        \n",
    "\n",
    "# Function that gets the number of model servings and the number of models for each serving and deploys them\n",
    "def create_model_servings(context, number_of_servings, number_of_models):\n",
    "    project_name = mlrun.get_current_project().name\n",
    "    \n",
    "    for serving_num in number_of_servings:\n",
    "        serving_fn = project.set_function(\"hub://v2_model_server\", f\"serving-func{str(serving_num)}\", kind=\"serving\", image=\"mlrun/mlrun\")\n",
    "        serving_fn.apply(mlrun.auto_mount())\n",
    "\n",
    "        # Add the models to the serving function's routing spec\n",
    "        for model_num in number_of_models:\n",
    "            model_name = f\"model_num_{str(model_num)}\"\n",
    "            serving_fn.add_model(model_name=model_name, model_path=f\"store://models/{project_name}/{model_name}:latest\")\n",
    "\n",
    "        # (OPTIONAL) Create a tracking policy \n",
    "        # tracking_policy = {'default_batch_intervals':\"*/1 * * * *\", \"default_batch_image\":\"mlrun/mlrun:1.6.0-rc15\", \"stream_image\":\"mlrun/mlrun:1.6.0-rc15\", \"default_controller_image\":\"mlrun/mlrun:1.6.0-rc15\"}\n",
    "\n",
    "        # Enable model monitoring (If you specified tracking_policy, pass it to 'tracking_policy' param)\n",
    "        serving_fn.set_tracking()\n",
    "        \n",
    "        serving_fn.deploy_function()\n",
    "        \n",
    "        \n",
    "\n",
    "project.enable_model_monitoring(base_period=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f132e7d-dfa0-44c6-8845-e70297e52903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evidently\n",
    "from evidently.renderers.notebook_utils import determine_template\n",
    "from evidently.report import Report\n",
    "from evidently.suite.base_suite import Suite\n",
    "from evidently.utils.dashboard import TemplateParams\n",
    "from evidently.ui.workspace import Workspace\n",
    "from evidently.ui.base import Project\n",
    "from evidently.metrics import (\n",
    "        ColumnDriftMetric,\n",
    "        ColumnSummaryMetric,\n",
    "        DatasetDriftMetric,\n",
    "        DatasetMissingValuesMetric,\n",
    "    )\n",
    "from evidently.test_preset import DataDriftTestPreset\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.ui.dashboards import (\n",
    "        CounterAgg,\n",
    "        DashboardConfig,\n",
    "        DashboardPanelCounter,\n",
    "        DashboardPanelPlot,\n",
    "        PanelValue,\n",
    "        PlotType,\n",
    "        ReportFilter,\n",
    "    )\n",
    "\n",
    "from typing import Optional\n",
    "from uuid import UUID\n",
    "\n",
    "\n",
    "def create_demo_project(workspace_path: str) -> tuple[Workspace, Project]:\n",
    "    workspace = Workspace.create(workspace_path)\n",
    "    project = _create_evidently_project(workspace)\n",
    "    return workspace, project\n",
    "\n",
    "\n",
    "def _create_evidently_project(workspace: Workspace, id: Optional[UUID] = None, project_name=\"default\", project_description=\"my description\") -> Project:\n",
    "    if id:\n",
    "        project = Project(\n",
    "            name=project_name,\n",
    "            description=project_description,\n",
    "            dashboard=DashboardConfig(name=project_name, panels=[]),\n",
    "            id=id,\n",
    "        )  # pyright: ignore[reportGeneralTypeIssues]\n",
    "        project = workspace.add_project(project)\n",
    "    else:\n",
    "        project = workspace.create_project(project_name)\n",
    "    project.description = project_description\n",
    "    project.dashboard.add_panel(\n",
    "        DashboardPanelCounter(\n",
    "            filter=ReportFilter(metadata_values={}, tag_values=[]),\n",
    "            agg=CounterAgg.NONE,\n",
    "            title=\"Income Dataset (iris)\",\n",
    "        )  # pyright: ignore[reportGeneralTypeIssues]\n",
    "    )\n",
    "    project.dashboard.add_panel(\n",
    "        DashboardPanelCounter(\n",
    "            title=\"Model Calls\",\n",
    "            filter=ReportFilter(metadata_values={}, tag_values=[]),\n",
    "            value=PanelValue(\n",
    "                metric_id=\"DatasetMissingValuesMetric\",\n",
    "                field_path=DatasetMissingValuesMetric.fields.current.number_of_rows,\n",
    "                legend=\"count\",\n",
    "            ),\n",
    "            text=\"count\",\n",
    "            agg=CounterAgg.SUM,\n",
    "            size=1,\n",
    "        )  # pyright: ignore[reportGeneralTypeIssues]\n",
    "    )\n",
    "    project.dashboard.add_panel(\n",
    "        DashboardPanelCounter(\n",
    "            title=\"Share of Drifted Features\",\n",
    "            filter=ReportFilter(metadata_values={}, tag_values=[]),\n",
    "            value=PanelValue(\n",
    "                metric_id=\"DatasetDriftMetric\",\n",
    "                field_path=\"share_of_drifted_columns\",\n",
    "                legend=\"share\",\n",
    "            ),\n",
    "            text=\"share\",\n",
    "            agg=CounterAgg.LAST,\n",
    "            size=1,\n",
    "        )  # pyright: ignore[reportGeneralTypeIssues]\n",
    "    )\n",
    "    project.dashboard.add_panel(\n",
    "        DashboardPanelPlot(\n",
    "            title=\"Dataset Quality\",\n",
    "            filter=ReportFilter(metadata_values={}, tag_values=[]),\n",
    "            values=[\n",
    "                PanelValue(\n",
    "                    metric_id=\"DatasetDriftMetric\",\n",
    "                    field_path=\"share_of_drifted_columns\",\n",
    "                    legend=\"Drift Share\",\n",
    "                ),\n",
    "                PanelValue(\n",
    "                    metric_id=\"DatasetMissingValuesMetric\",\n",
    "                    field_path=DatasetMissingValuesMetric.fields.current.share_of_missing_values,\n",
    "                    legend=\"Missing Values Share\",\n",
    "                ),\n",
    "            ],\n",
    "            plot_type=PlotType.LINE,\n",
    "        )  # pyright: ignore[reportGeneralTypeIssues]\n",
    "    )\n",
    "    project.save()\n",
    "    return project\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99161721-b234-4995-85a3-3340c7bb507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A v3io path, for example in \"/v3io/bigdata/evidently-folder\"\n",
    "evidently_project_path = \"\"\n",
    "workspace, project_evid = create_demo_project(evidently_project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03e019-694b-448c-9522-63cf8458f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile custom_evidently_app.py\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from mlrun.common.schemas.model_monitoring.constants import (\n",
    "    ResultKindApp,\n",
    "    ResultStatusApp,\n",
    ")\n",
    "from mlrun.model_monitoring.application import ModelMonitoringApplicationResult\n",
    "from mlrun.model_monitoring.evidently_application import (\n",
    "    _HAS_EVIDENTLY,\n",
    "    EvidentlyModelMonitoringApplicationBase,\n",
    ")\n",
    "if _HAS_EVIDENTLY:\n",
    "    from evidently.metrics import (\n",
    "        ColumnDriftMetric,\n",
    "        ColumnSummaryMetric,\n",
    "        DatasetDriftMetric,\n",
    "        DatasetMissingValuesMetric,\n",
    "    )\n",
    "    from evidently.metric_preset import DataQualityPreset\n",
    "    from evidently.report import Report\n",
    "    from evidently.test_preset import DataDriftTestPreset\n",
    "    from evidently.test_suite import TestSuite\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class CustomEvidentlyMonitoringApp(EvidentlyModelMonitoringApplicationBase):\n",
    "    name = \"my-custom-evidently-class\"\n",
    "\n",
    "    def _lazy_init(self, *args, **kwargs) -> None:\n",
    "        super()._lazy_init(*args, **kwargs)\n",
    "        self._init_iris_data()\n",
    "\n",
    "    def _init_iris_data(self) -> None:\n",
    "        iris = load_iris()\n",
    "        self.columns = [\n",
    "            \"sepal_length_cm\",\n",
    "            \"sepal_width_cm\",\n",
    "            \"petal_length_cm\",\n",
    "            \"petal_width_cm\",\n",
    "        ] \n",
    "        self.train_set = pd.DataFrame(iris.data, columns=self.columns)\n",
    "\n",
    "            \n",
    "    def do_tracking(\n",
    "    self,\n",
    "    application_name: str,\n",
    "    sample_df_stats: pd.DataFrame,\n",
    "    feature_stats: pd.DataFrame,\n",
    "    sample_df: pd.DataFrame,\n",
    "    start_infer_time: pd.Timestamp,\n",
    "    end_infer_time: pd.Timestamp,\n",
    "    latest_request: pd.Timestamp,\n",
    "    endpoint_id: str,\n",
    "    output_stream_uri: str,\n",
    ") -> ModelMonitoringApplicationResult:\n",
    "        self.context.logger.info(\"Running evidently app\")\n",
    "\n",
    "        # self._init_iris_data()\n",
    "        sample_df = sample_df[self.columns]\n",
    "\n",
    "        # Create evidently reports\n",
    "        data_drift_report = self.create_report(sample_df, end_infer_time)\n",
    "        self.evidently_workspace.add_report(\n",
    "            self.evidently_project_id, data_drift_report\n",
    "        )\n",
    "        \n",
    "        data_quality_report = Report(metrics=[\n",
    "            DataQualityPreset(),\n",
    "        ])\n",
    "        data_quality_report.run(reference_data=self.train_set, current_data=sample_df)\n",
    "\n",
    "        # Create evidently test suite\n",
    "        data_drift_test_suite = self.create_test_suite(sample_df, end_infer_time)\n",
    "        self.evidently_workspace.add_test_suite(\n",
    "            self.evidently_project_id, data_drift_test_suite\n",
    "        )\n",
    "        \n",
    "        # Log the objects in iguazio\n",
    "        self.log_evidently_object(data_drift_report, f\"report_{str(end_infer_time)}\")\n",
    "        self.log_evidently_object(data_drift_test_suite, f\"suite_{str(end_infer_time)}\")\n",
    "        self.log_evidently_object(data_quality_report, f\"data_quality_report_{str(end_infer_time)}\")\n",
    "        \n",
    "        # Log the dashboard in iguazio\n",
    "        self.log_project_dashboard(None, end_infer_time + datetime.timedelta(minutes=1))\n",
    "\n",
    "        self.context.logger.info(\"Logged evidently objects\")\n",
    "        \n",
    "        return ModelMonitoringApplicationResult(\n",
    "            application_name=self.name,\n",
    "            endpoint_id=endpoint_id,\n",
    "            start_infer_time=start_infer_time,\n",
    "            end_infer_time=end_infer_time,\n",
    "            result_name=\"data_drift_test\",\n",
    "            result_value=0.5,\n",
    "            result_kind=ResultKindApp.data_drift,\n",
    "            result_status=ResultStatusApp.potential_detection,\n",
    "        )\n",
    "    \n",
    "    # Function that creates an evidently report\n",
    "    def create_report(\n",
    "    self, sample_df: pd.DataFrame, schedule_time: pd.Timestamp\n",
    ") -> \"Report\":\n",
    "        metrics = [\n",
    "            DatasetDriftMetric(),\n",
    "            DatasetMissingValuesMetric(),\n",
    "        ]\n",
    "        for col_name in self.columns:\n",
    "            metrics.extend(\n",
    "                [\n",
    "                    ColumnDriftMetric(column_name=col_name, stattest=\"wasserstein\"),\n",
    "                    ColumnSummaryMetric(column_name=col_name),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        data_drift_report = Report(\n",
    "            metrics=metrics,\n",
    "            timestamp=schedule_time,\n",
    "        )\n",
    "\n",
    "        data_drift_report.run(reference_data=self.train_set, current_data=sample_df)\n",
    "        return data_drift_report\n",
    "\n",
    "    # Function that creates an evidently test suite\n",
    "    def create_test_suite(\n",
    "        self, sample_df: pd.DataFrame, schedule_time: pd.Timestamp\n",
    "    ) -> \"TestSuite\":\n",
    "        data_drift_test_suite = TestSuite(\n",
    "            tests=[DataDriftTestPreset()],\n",
    "            timestamp=schedule_time,\n",
    "        )\n",
    "\n",
    "        data_drift_test_suite.run(reference_data=self.train_set, current_data=sample_df)\n",
    "        return data_drift_test_suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89bc5b-7cc0-4a80-877d-7582fb1e117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the monitoring serving function\n",
    "model_monitoring_func = project.set_model_monitoring_function(func=\"custom_evidently_app.py\", application_class=\"CustomEvidentlyMonitoringApp\", name=\"custom-evidently-class\",\n",
    "                                      image=\"mlrun/mlrun\", requirements=[\"evidently==0.4.11\"], evidently_workspace_path=workspace.path,\n",
    "                                     evidently_project_id=str(project_evid.id))\n",
    "\n",
    "model_monitoring_func.apply(mlrun.auto_mount())\n",
    "\n",
    "model_monitoring_func.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca5dbc-8285-41a0-9a93-0108b1cae64a",
   "metadata": {},
   "source": [
    "## log dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8742699-709c-47ac-99fe-4b8660fe9efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% writefile func.py\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Function that logs a large dataset\n",
    "def func_dataset(context, num_rows, num_columns):\n",
    "        \n",
    "    # Generate random data\n",
    "    data = np.random.rand(num_rows, num_columns)\n",
    "\n",
    "    # Create column names\n",
    "    columns = [f'Column_{i}' for i in range(num_columns)]\n",
    "\n",
    "    # Create the DataFrame and give it a name with a random int suffix\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    context.log_dataset(f\"mydf_{str(random.randint(0,num_rows))}\", df)\n",
    "        \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14136025-15ac-4f30-a0cb-77be8b57ea1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlrun-base",
   "language": "python",
   "name": "conda-env-mlrun-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
